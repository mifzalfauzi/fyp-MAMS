# # -*- coding: utf-8 -*-
# """Copy of  TTTU4086 PROJECT - Dedicated NLP Pipeline with OpenAI Integration

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/117LE_6s_mugkK4loOipO0rXF01-Woo6O

# #**TTTU4086 PROJECT - Dedicated NLP Pipeline with OpenAI Integration**
# """



# """# **Task 1 : Upload Files**"""

# import pandas as pd
# from google.colab import files

# # Upload CSV files
# uploaded = files.upload()

# # Read the uploaded CSV files into Pandas DataFrames
# dfs = {}  # Dictionary to store DataFrames
# for filename in uploaded.keys():
#     with open(filename, 'r') as file:
#         df = pd.read_csv(file)
#         variable_name = filename.replace('.csv', '_df')  # Generate variable name based on filename
#         dfs[variable_name] = df  # Assign variable name to DataFrame

# # Check if variable names are stored correctly
# for variable_name in dfs.keys():
#     print(f"Variable name: {variable_name}")
#     # Try accessing the DataFrame using the variable name
#     try:
#         df = dfs[variable_name]
#         print("DataFrame:")
#         print(df.head())  # Display the DataFrame
#     except KeyError:
#         print("DataFrame not found for this variable name.")

# from tabulate import tabulate
# # Display the loaded DataFrames in a tabulated format
# for filename, df in dfs.items():
#     print(f"DataFrame from file '{filename}':")

#     print(tabulate(df.head(20), headers='keys', tablefmt='psql'))  # Display the DataFrame in a tabulated format


#     df.head(20)

# for filename, df in dfs.items():
#     num_data =len(df)
#     print(num_data)

# """# **Task 2 : Data Preprocessing**"""

# import nltk
# nltk.download('stopwords')
# nltk.download('vader_lexicon')

# from nltk.corpus import stopwords
# import re
# import io

# # Get the first key in the dictionary
# first_key = list(dfs.keys())[0]

# # Print the first key
# print("First key:", first_key)

# first_df_key = next(iter(dfs.keys()))  # Get the first key in the dictionary
# first_df = dfs[first_df_key]  # Select the DataFrame corresponding to the first uploaded file


# # Drop rows with missing values
# first_df_cleaned = first_df.dropna()

# # Print the shape of the cleaned DataFrame to verify the number of rows remaining
# print("Shape of cleaned DataFrame:", first_df_cleaned.shape)

# # Define stopwords
# stop_words = set(stopwords.words('english'))

# # Define function to clean text
# def clean_text(text):
#     # Convert text to lowercase
#     text = text.lower()
#     # Remove special characters except apostrophes
#     text = re.sub(r'[^a-zA-Z0-9\s\']', '', text)
#     # Remove stopwords
#     text = ' '.join(word for word in text.split() if word not in stop_words)
#     return text

# # Apply text cleaning function to the 'text' column
# first_df_cleaned['cleaned_text'] = first_df_cleaned['content'].apply(clean_text)

# # Include the 'id' column in the cleaned DataFrame
# first_df_cleaned = first_df_cleaned[['id', 'cleaned_text','roles']]

# #List of columns to keep
# columns_to_keep = ['id','cleaned_text','roles']

# #Drop columns not in the list
# first_df_cleaned = first_df_cleaned[columns_to_keep]

# # Display the cleaned DataFrame
# first_df_cleaned.head(20)

# """# **Task 3 : Relate Tables**"""

# # Get the keys of the dictionary
# keys = list(dfs.keys())

# # Assign variables to the second and third keys
# second_key = keys[1]
# third_key = keys[2]

# # Assign variables to the DataFrames corresponding to the second and third keys
# second_df = dfs[second_key]
# third_df = dfs[third_key]

# # Display the variables\

# print("\nFirst DataFrame:")
# display(first_df_cleaned.head(20))

# print("Second DataFrame:")
# display(second_df.head(20))

# print("\nThird DataFrame:")
# display(third_df.head(20))

# # Assuming dataset_df, dataset_labels_df, and labels_df are your DataFrames containing the respective tables

# print("First DataFrame columns:", first_df_cleaned.columns)
# print("Second DataFrame columns:", second_df.columns)

# # Merge dataset and dataset_labels on the common ID column
# merged_df = pd.merge(first_df_cleaned, second_df, left_on='id', right_on='datasetentry_id')

# # # Merge the resulting DataFrame with labels on the common ID column
# final_merged_df = pd.merge(merged_df, third_df, left_on='label_id', right_on='id')

# # Print the first few rows of the merged DataFrame
# print("First few rows of the merged DataFrame:")
# display(final_merged_df.head())

# # Optionally, you can also print the column names to understand the structure of the merged DataFrame
# print("\nColumn names of the merged DataFrame:")
# display(final_merged_df.columns)

# # Define the new column names
# new_column_names = {
#     'id_y': 'related_id',
#     'id_x': 'content_id',
#     # Add more mappings as needed
# }

# # Rename the columns
# final_merged_df.rename(columns=new_column_names, inplace=True)

# # Sort the DataFrame by a specific column in ascending order
# final_merged_df_sorted = final_merged_df.sort_values(by='related_id', ascending=True)

# # Print the first few rows of the merged DataFrame
# print("First few rows of the merged DataFrame:")
# display(final_merged_df.head(997))

# # Drop unnecessary columns
# final_merged_df.drop(columns=['datasetentry_id', 'id','label_id','content_id', 'related_id'], inplace=True)

# # Drop duplicates based on specific columns
# final_merged_df.drop_duplicates(subset=['cleaned_text', 'roles', 'name'], inplace=True)


# # Print the first few rows of the merged DataFrame
# print("First few rows of the merged DataFrame:")
# display(final_merged_df.head(1000))

# """# **Task 4 : Data Preparation for Modeling**

# **Between 'cleaned_text' and 'name'**
# """

# # Step 1: Split Data into Features and Labels

# X = final_merged_df['cleaned_text'] + ' ' + final_merged_df['roles']# Assuming 'user_query_column' is the column containing user queries
# y = final_merged_df['name']  # Assuming 'category_label_column' is the column containing category labels

# # Step 2: Vectorization (TF-IDF)
# from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer

# # Initialize the TF-IDF vectorizer
# tfidf_vectorizer = TfidfVectorizer()

# # Fit-transform the user query text data to generate TF-IDF vectors
# X_tfidf = tfidf_vectorizer.fit_transform(X)

# # Check the shape of the TF-IDF matrix
# print("Shape of TF-IDF matrix:", X_tfidf.shape)

# import matplotlib.pyplot as plt
# from sklearn.decomposition import PCA
# from sklearn.preprocessing import LabelEncoder
# from sklearn.model_selection import train_test_split

# # Perform PCA to reduce the dimensionality of the TF-IDF matrix
# pca = PCA(n_components=2)
# X_pca = pca.fit_transform(X_tfidf.toarray())

# # Convert category labels to numeric values
# label_encoder = LabelEncoder()
# y_encoded = label_encoder.fit_transform(y)

# # Visualize the TF-IDF matrix in 2D
# plt.figure(figsize=(10, 6))
# scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_encoded, cmap='viridis')
# plt.title('2D Visualization of TF-IDF Matrix')
# # Axis labels
# plt.xlabel('User Query')
# plt.ylabel('Category')

# plt.colorbar(scatter, label='Category Label')
# plt.grid(True)
# plt.show()

# """# **Machine Learning**"""

# final_merged_df['name'].value_counts()

# # Split the data into training and testing sets
# X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)

# # Optionally, split the training set into training and validation sets
# # X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# """**CLASSIFIER - Linear Regression**"""

# #Build the classifier model - Logistic Regression

# from sklearn.metrics import accuracy_score, classification_report
# from sklearn.linear_model import LogisticRegression

# scikit_log_reg = LogisticRegression(verbose=1, solver='liblinear', random_state=0, C=5, penalty='l2', max_iter=1000)
# model_LR = scikit_log_reg.fit(X_train, y_train)
# lr_predicted = model_LR.predict(X_test)

# # Calculate accuracy
# accuracy = accuracy_score(y_test, lr_predicted)
# print("Linear Regression with TFIDF Accuracy:", accuracy)

# print("")
# classification_report_result = classification_report(y_test, lr_predicted)
# print(f"Classification Report:\n{classification_report_result}")

# """**CLASSIFIER - Support Vector Machine**"""

# #Build the classifier model - SVM
# from sklearn.metrics import accuracy_score, classification_report
# from sklearn.svm import SVC
# import joblib
# import pickle

# svm_classifier = SVC(kernel='linear', C=1.0, random_state=0)
# model_svm = svm_classifier.fit(X_train, y_train)
# svm_predicted = model_svm.predict(X_test)
# print("SVM Accuracy with TFIDF:", accuracy_score(y_test, svm_predicted))

# print("")
# classification_report_result = classification_report(y_test, svm_predicted)
# print(f"Classification Report:\n{classification_report_result}")

# # Save the model using joblib
# joblib.dump(model_svm, 'model.pkl')


# # Load the pickled file
# with open('model.pkl', 'rb') as file:
#     loaded_model = pickle.load(file)

# # Print out the contents of the loaded model
# print(loaded_model)

# """**CLASSIFIER - Naive Bayes**"""

# #Build the classifier model - Naive Bayes
# from sklearn.metrics import accuracy_score, classification_report
# from sklearn.naive_bayes import MultinomialNB

# model_nb = MultinomialNB().fit(X_train, y_train)
# nb_predicted = model_nb.predict(X_test)
# print("MultinomialNB Accuracy with TFIDF:", accuracy_score(y_test, nb_predicted))

# print("")
# classification_report_result = classification_report(y_test, nb_predicted)
# print(f"Classification Report:\n{classification_report_result}")

# accuracy_scores = [accuracy_score(y_test, lr_predicted),
#                    accuracy_score(y_test, nb_predicted),
#                    accuracy_score(y_test, svm_predicted)]

# models = ['Logistic Regression', 'Naive Bayes', 'SVM']

# # Plotting a bar chart
# plt.bar(models, accuracy_scores, color=['blue', 'green', 'orange'])
# plt.title('Model Comparison - Accuracy Scores')
# plt.xlabel('Model')
# plt.ylabel('Accuracy Score')
# plt.ylim(0, 1)
# plt.show()

# """# **IMPROVING THE RESULT**

# **UPSAMPLING**
# """

# final_merged_df['name'].value_counts()

# from sklearn.utils import resample
# import pandas as pd

# # Retrieve class distribution from the DataFrame
# class_distribution = final_merged_df['name'].value_counts()

# # Define the target number of samples for each category
# target_samples = class_distribution.max()

# # Upsample each minority category to match its original count
# upsampled_categories = []
# for category, count in class_distribution.items():
#     category_class = final_merged_df[final_merged_df['name'] == category]
#     if count < target_samples:
#         upsampled_class = resample(category_class, replace=True, n_samples=target_samples - count, random_state=42)
#         upsampled_categories.append(upsampled_class)

# # Combine the upsampled minority categories with the original dataset
# df_balanced = pd.concat([final_merged_df] + upsampled_categories)

# # Check the balanced class distribution
# print(df_balanced['name'].value_counts())

# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
# from sklearn.model_selection import train_test_split

# # Function for feature extraction
# def extract_features(final_merged_df, field, training_data, testing_data, type="binary"):
#     if "binary" in type:
#         vectorizer = CountVectorizer(binary=True, max_df=0.95)
#     elif "counts" in type:
#         vectorizer = CountVectorizer(binary=False, max_df=0.95)
#     else:
#         vectorizer = TfidfVectorizer(use_idf=True, max_df=0.95)

#     vectorizer.fit(training_data[field].values)
#     train_feature_set = vectorizer.transform(training_data[field].values)
#     test_feature_set = vectorizer.transform(testing_data[field].values)

#     return train_feature_set, test_feature_set, vectorizer

# print(final_merged_df.head())  # Display the first 5 rows
# print(df.head(10))  # Display the first 10 rows

# # Define text column and feature representation type
# field = 'cleaned_text'
# feature_rep = 'tf'

# # Split data into training and testing sets
# X_train, X_test, y_train, y_test = train_test_split(final_merged_df[field], final_merged_df['name'], test_size=0.2, random_state=42)

# # Extract features
# X_train_features, X_test_features, feature_transformer = extract_features(final_merged_df, field, X_train.to_frame(), X_test.to_frame(), type=feature_rep)

# # Check the balanced class distribution
# print(df_balanced['name'].value_counts())

# """# **Models**"""

# #build the classifier model - logistic regression
# from sklearn.metrics import accuracy_score, classification_report
# from sklearn.linear_model import LogisticRegression

# scikit_log_reg = LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000)
# model_LR=scikit_log_reg.fit(X_train_features,y_train)
# lr_predicted= model_LR.predict(X_test_features)
# print("Linear Regression with TFIDF:",accuracy_score(y_test, lr_predicted))

# print("")
# classification_report_result = classification_report(y_test, lr_predicted)
# print(f"Classification Report:\n{classification_report_result}")

# #build the classifier model - naive bayes
# from sklearn.naive_bayes import MultinomialNB
# model_nb = MultinomialNB().fit(X_train_features, y_train)
# nb_predicted= model_nb.predict(X_test_features)
# print("MultinomialNB Accuracy with TFIDF:",accuracy_score(y_test, nb_predicted))

# print("")
# classification_report_result = classification_report(y_test, nb_predicted)
# print(f"Classification Report:\n{classification_report_result}")

# #build the classifier model - SVM
# from sklearn.svm import SVC
# svm_classifier = SVC(kernel='linear', C=1.0, random_state=0)
# model_svm = svm_classifier.fit(X_train_features, y_train)
# svm_predicted = model_svm.predict(X_test_features)
# print("SVM Accuracy with TFIDF:", accuracy_score(y_test, svm_predicted))

# print("")
# classification_report_result = classification_report(y_test, svm_predicted)
# print(f"Classification Report:\n{classification_report_result}")

# """**MODEL COMPARISON**"""

# import matplotlib.pyplot as plt

# accuracy_scores = [accuracy_score(y_test, lr_predicted),
#                    accuracy_score(y_test, nb_predicted),
#                    accuracy_score(y_test, svm_predicted)]

# models = ['Logistic Regression', 'Naive Bayes', 'SVM']

# # Plotting a bar chart
# plt.bar(models, accuracy_scores, color=['blue', 'green', 'orange'])
# plt.title('Model Comparison - Accuracy Scores')
# plt.xlabel('Model')
# plt.ylabel('Accuracy Score')
# plt.ylim(0, 1)
# plt.show()

# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
# from sklearn.linear_model import LogisticRegression
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import accuracy_score
# from sklearn.ensemble import VotingClassifier

# # Step 3: Split Data into Training and Testing Sets
# X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)

# # Step 4: Initialize Base Estimators
# logistic_regression = LogisticRegression()
# svm_classifier = SVC(kernel='linear', C=1.0, random_state=42)
# # naive_bayes = MultinomialNB()

# # Step 5: Initialize Voting Classifier
# voting_classifier = VotingClassifier(estimators=[('lr', logistic_regression), ('svm', svm_classifier)], voting='hard')

# # Step 6: Train Voting Classifier
# voting_classifier.fit(X_train, y_train)

# # Step 7: Make Predictions
# y_pred = voting_classifier.predict(X_test)

# # Step 8: Evaluate Model
# accuracy = accuracy_score(y_test, y_pred)
# print("Accuracy:", accuracy)
# classification_report_result = classification_report(y_test, y_pred)
# print("Classification Report:\n", classification_report_result)