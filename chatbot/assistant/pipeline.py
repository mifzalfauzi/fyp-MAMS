# -*- coding: utf-8 -*-
"""Pipeline for NLP tasks including data cleaning, processing, and prediction."""

# # -*- coding: utf-8 -*-
# """Copy of  TTTU4086 PROJECT - Dedicated NLP Pipeline with OpenAI Integration

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/117LE_6s_mugkK4loOipO0rXF01-Woo6O

# #**TTTU4086 PROJECT - Dedicated NLP Pipeline with OpenAI Integration**

# """


import matplotlib.pyplot as plt
import nltk
import re
import os
import pandas as pd

#-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

from sklearn.model_selection import train_test_split
from sklearn.utils import resample
from sklearn.metrics import accuracy_score, classification_report
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

#---------------------------------------------------------------------------------------------------------------------------------------

# ----------------------------------------"""# **TASK 1 : READING FILES**"""------------------------------------------------------------------------------------
"""READING FILES"""
# Assuming this view function is part of a Django app's views.py file


 # Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('vader_lexicon')

# Define the function to clean text
def clean_text(text):
    stop_words = set(stopwords.words('english'))
    text = text.lower()
    text = re.sub(r'[^a-zA-Z0-9\s\']', '', text)
    text = ' '.join(word for word in text.split() if word not in stop_words)
    return text   

# Define the function to read, join, and clean the data
def check():
    
    # Specify the path to the datasets folder
    datasets_folder = os.path.join(os.path.dirname(__file__), 'datasets')

    # Read the CSV files into Pandas DataFrames
    dataset_files = os.listdir(datasets_folder)
    dfs = {}
    for filename in dataset_files:
        if filename.endswith('.csv'):
            try:
                df = pd.read_csv(os.path.join(datasets_folder, filename))
                dfs[filename.split('.')[0]] = df
            except Exception as e:
                print(f"Error reading file '{filename}': {e}")

    # Check if required tables are found
    required_tables = ['finalyearproject_table_assistant_datasetentry', 'finalyearproject_table_assistant_datasetentry_labels', 'finalyearproject_table_assistant_label']


# ----------------------------------------"""# **TASK 2 : CLEANING & PREPROCESS DATA**"""------------------------------------------------------------------------------------


    if all(table in dfs for table in required_tables):
        # Join the tables
        merged_df = dfs['finalyearproject_table_assistant_datasetentry'].merge(dfs['finalyearproject_table_assistant_datasetentry_labels'], left_on='id', right_on='datasetentry_id')
        merged_df = merged_df.merge(dfs['finalyearproject_table_assistant_label'], left_on='label_id', right_on='id')
        
        # Clean the text data
        merged_df['cleaned_text'] = merged_df['content'].apply(clean_text)
        
        # Select relevant columns
        cleaned_df = merged_df[['cleaned_text', 'roles', 'name']]

        # Drop duplicates based on specific columns
        cleaned_df.drop_duplicates(subset=['cleaned_text', 'roles', 'name'], inplace=True)

        return cleaned_df
    else:
        print("One or more required tables not found.")
        return None

# Call the function to process the data
cleaned_df = check()

# ----------------------------------------"""# **TASK 3 : DATA PREPARATION  **"""------------------------------------------------------------------------------------                 
"""DATA PREPARATION"""


def split_data(df):
    """
    Split DataFrame into features (X) and labels (y).
    
    Parameters:
    df (DataFrame): DataFrame containing the data.

    Returns:
    X (Series): Series containing the features.
    y (Series): Series containing the labels.
    """
    X = df['cleaned_text'] + ' ' + df['roles']
    y = df['name']
    return X, y


if cleaned_df is not None:
    X, y = split_data(cleaned_df)
    print(X.head())
    print(y.head())
    print("")

def upsample_minority_classes(df):
    """
    Upsample minority classes in the DataFrame to balance class distribution.

    Parameters:
    df (DataFrame): DataFrame containing the data.

    Returns:
    df_balanced (DataFrame): DataFrame with balanced class distribution after upsampling.
    """
    # Retrieve class distribution from the DataFrame
    class_distribution = df['name'].value_counts()

    # Define the target number of samples for each category
    target_samples = class_distribution.max()

    # Upsample each minority category to match its original count
    upsampled_categories = []
    for category, count in class_distribution.items():
        category_class = df[df['name'] == category]
        if count < target_samples:
            upsampled_class = resample(category_class, replace=True, n_samples=target_samples - count, random_state=42)
            upsampled_categories.append(upsampled_class)

    # Combine the upsampled minority categories with the original dataset
    df_balanced = pd.concat([df] + upsampled_categories)

    return df_balanced

# Display value counts for the 'name' column
print("Value counts for 'name' column:")
print(cleaned_df['name'].value_counts().to_string())
print("")

# Class distribution:
df_balanced = upsample_minority_classes(cleaned_df)
print("Balanced class distribution after upsampling:")
print(df_balanced['name'].value_counts())


def tfidf_vectorize(X):
    """
    Vectorize the input text data using TF-IDF.

    Parameters:
    text_data (Series): Series containing the text data to vectorize.

    Returns:
    X_tfidf (sparse matrix): TF-IDF matrix representation of the input text data.
    tfidf_vectorizer (TfidfVectorizer): Fitted TF-IDF vectorizer.
    """
    # Initialize the TF-IDF vectorizer
    tfidf_vectorizer = TfidfVectorizer()

    # Fit-transform the text data to generate TF-IDF vectors
    X_tfidf = tfidf_vectorizer.fit_transform(X)

    return X_tfidf, tfidf_vectorizer


X_tfidf, tfidf_vectorizer = tfidf_vectorize(X)
print("Shape of TF-IDF matrix:", X_tfidf.shape)
print("")


def split_train_test(X, y, test_size=0.2, random_state=42):
    """
    Split the dataset into training and testing sets.

    Parameters:
    X (Series): Series containing the features.
    y (Series): Series containing the labels.
    test_size (float): The proportion of the dataset to include in the test split (default is 0.2).
    random_state (int): Controls the shuffling applied to the data before splitting (default is 42).

    Returns:
    X_train (Series): Training features.
    X_test (Series): Testing features.
    y_train (Series): Training labels.
    y_test (Series): Testing labels.
    """
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)
    return X_train, X_test, y_train, y_test

# Example usage:
X_train, X_test, y_train, y_test = split_train_test(X, y)
print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)
print("")


# ----------------------------------------"""# **Task 4 : Machine Learning**"""------------------------------------------------------------------------------------
"""Machine Learning"""

# Step 3: Split Data into Training and Testing Sets
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)

# Step 4: Initialize Base Estimators (IN THIS CASE -  THERE ARE 3 BASE ESTIMATORS - SVM, KNN AND LOGISTIC REGRESSION)
logistic_regression = LogisticRegression()
svm_classifier = SVC(kernel='linear', C=1.0, probability=True)
knn_classifier = KNeighborsClassifier()

# Step 5: Train Base Estimators
logistic_regression.fit(X_train, y_train)
svm_classifier.fit(X_train, y_train)
knn_classifier.fit(X_train, y_train)

# Step 6: Get Probability Estimates for Base Estimators
logistic_regression_proba = logistic_regression.predict_proba(X_test)
svm_classifier_proba = svm_classifier.predict_proba(X_test)
knn_classifier_proba = knn_classifier.predict_proba(X_test)  # Corrected variable name

# Step 7: Make Predictions for Base Estimators
logistic_regression_pred = logistic_regression.predict(X_test)
svm_classifier_pred = svm_classifier.predict(X_test)
knn_classifier_pred = knn_classifier.predict(X_test)  # Corrected variable name

# Step 8: Evaluate Base Estimators
logistic_regression_accuracy = accuracy_score(y_test, logistic_regression_pred)
svm_classifier_accuracy = accuracy_score(y_test, svm_classifier_pred)
knn_classifier_accuracy = accuracy_score(y_test, knn_classifier_pred)

print("Logistic Regression Accuracy:", logistic_regression_accuracy)
print("SVM Accuracy:", svm_classifier_accuracy)
print("KNN Classifier Accuracy:", knn_classifier_accuracy)

# Generate Classification Reports for Base Classifiers
print("Classification Report for Logistic Regression:")
print(classification_report(y_test, logistic_regression_pred))

print("Classification Report for SVM:")
print(classification_report(y_test, svm_classifier_pred))

print("Classification Report for KNN Classifier:")
print(classification_report(y_test, knn_classifier_pred))


# Step 9: Combine Probability Estimates of Base Estimators
combined_proba = (logistic_regression_proba + knn_classifier_proba) / 2.0

# Step 10: Initialize Voting Classifier with soft voting
voting_classifier = VotingClassifier(estimators=[('lr', logistic_regression), ('knn', knn_classifier), ('svm', svm_classifier)], voting='soft')

# Step 11: Train Voting Classifier
voting_classifier.fit(X_train, y_train)

# Step 12: Get Probability Estimates
voting_proba = voting_classifier.predict_proba(X_test)

# Step 13: Make Predictions
voting_pred = voting_classifier.predict(X_test)

# Step 14: Evaluate Voting Classifier
voting_accuracy = accuracy_score(y_test, voting_pred)
print("Voting Classifier Accuracy:", voting_accuracy)

#Generate Classification Report for Voting Classifier
print("Classification Report for Voting Classifier:")
print(classification_report(y_test, voting_pred))